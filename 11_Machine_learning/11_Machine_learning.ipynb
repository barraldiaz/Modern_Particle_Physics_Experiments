{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/akalinow/Modern_Particle_Physics_Experiments/blob/2022_2023/11_Machine_learning/11_Machine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - part II "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Color printing\n",
    "from termcolor import colored\n",
    "\n",
    "#General data operations library\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, mean_squared_error\n",
    "\n",
    "#Models\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Increase plots font size\n",
    "params = {'legend.fontsize': 'xx-large',\n",
    "          'figure.figsize': (10, 7),\n",
    "         'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'xx-large',\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# An universal approximation theorem [Cybenko, 1989](https://link.springer.com/article/10.1007/BF02551274):\n",
    "\n",
    "Let's define a ``neuron`` function on $R^{n} \\rightarrow R$:\n",
    "$$\n",
    "\\huge{\n",
    " f_{k}(\\theta_{k}, x) = A(\\sum_{i=1}^{N} \\theta_{i,k} x_{i} + \\beta_{k})\n",
    "}\n",
    "$$\n",
    "\n",
    "where `A` - activation function: any function of a single argument that fulfills requirements:\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    " \\lim_{x\\rightarrow -\\infty} f(x) \\rightarrow 0 \\\\\n",
    " \\lim_{x\\rightarrow +\\infty} f(x) \\rightarrow 1 \\\\\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Theorem**: every continuous function on $R^{n} \\rightarrow R$ can be approximated in basis of neural functions (=one layer of neurons).\n",
    "\n",
    "$$\n",
    "\\huge{\n",
    "y(x) \\simeq \\sum_{k} w_{k} f_{k}(\\theta_{k}, x)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions:\n",
    "\n",
    "* siogmoid: \n",
    "$$\n",
    "\\huge{\n",
    "f(x) = \\frac{1}{1+e^{-x}}\n",
    "}\n",
    "$$\n",
    "* rectified linear unit (ReLu):\n",
    "$$\n",
    "\\huge{\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "    0       & \\quad \\text{if } x<0\\\\\n",
    "    x  & \\quad \\text{if } x \\geq 0\n",
    "  \\end{cases}\n",
    "}\n",
    "$$\n",
    "\n",
    "**Note**: ReLu function does not fulfill the Cybenko theorem, but there are other theorems showing that networks with ReLu are also universal approximators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# The training\n",
    "\n",
    "How the parameters $(\\theta_{k}, w_{k})$ of the model are found? \n",
    "\n",
    "`We calculate gradient vector of the loss function wrt. the parameters, and go opposite to the gradient to the minimum of the loss function.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The algorithm is as follows:\n",
    "\n",
    "1) define starting values for $\\theta$ parameters: `init_theta`. Ussually `init_theta` are taken random.\n",
    "\n",
    "2) select a sub sample of examples to be used for the loss function calculation: a `batch`\n",
    "\n",
    "3) calculate the loss function gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "4) update the $\\theta$ parameter proportionally to the gradient, with proportionality parameter $\\alpha$ (\"learning rate\"):\n",
    "$$\n",
    "$$\n",
    "$$\n",
    "\\huge{\n",
    " \\theta_{new} = \\theta_{old} - \\alpha \\cdot \\nabla_{\\theta} L\n",
    "}\n",
    "$$\n",
    "\n",
    "In reality more complicated algoriuthms are used, but the general idea is the same.\n",
    "\n",
    "5) loop with batches through the whole dataset. One pass through the whole dataset is called an `epoch`\n",
    "\n",
    "6) repeat reading the whole dataset many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is implemented within any package used for machine learning. The most widealy used packages are:\n",
    "\n",
    "* [scikit-learn](https://scikit-learn.org/stable/) - \"Simple and efficient tools for predictive data analysis\" - a very good library for begginers. Contains a large set of ML algrotihms, with easy to use interface for training and inference \n",
    "* [PyTorch](https://pytorch.org/) - an open source machine learning framework developed by Meta AI (Facebook)\n",
    "* [TensorFlow](https://www.tensorflow.org/) - Google machine learning framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression example\n",
    "\n",
    "**Please:**\n",
    "\n",
    "prepare a dataset for a 1D regression task: \n",
    "$$\n",
    "\\huge{\n",
    "f(x) = y\n",
    "}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$X$ - features, in range $[0, 2\\pi]$\n",
    "\n",
    "$y$ - targets\n",
    "\n",
    "* prepare a list of random 10 000 values of $x$. Make sure the shape of the X is `(-1,1)`\n",
    "* prepare a set of y values, definded as $y = \\sin(x)$\n",
    "* split the dataset into training and testng subsets in proportions 8:2 - use the `train_test_split()` function - check documentation how to use it with X and Y arrays\n",
    "* plot y vs x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BEGIN_SOLUTION\n",
    "nPoints = 10000\n",
    "minX = 0.0\n",
    "maxX = 2.0*np.pi\n",
    "X = np.random.default_rng().uniform(minX, maxX, nPoints)\n",
    "y = np.sin(X)\n",
    "\n",
    "X = np.reshape(X, (-1,1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(colored(\"Train labels shape:\",\"blue\"), y_train.shape)\n",
    "print(colored(\"Test labels shape:\",\"blue\"), y_test.shape)\n",
    "\n",
    "print(colored(\"Train features shape:\",\"blue\"), X_train.shape)\n",
    "print(colored(\"Test features shape:\",\"blue\"), X_test.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.plot(X_train,y_train, \"bo\", label=\"training data\")\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y');\n",
    "ax.legend()\n",
    "##END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a simple neural netowrk (NN) sometimes called multi layer perceptron (MLP) to reconstruct the target values.\n",
    "The model uses a network of connected units (neurons), hence the name: neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a model object.\n",
    "hidden_layer_sizes = (1)\n",
    "model =  MLPRegressor(random_state=1, max_iter=500, hidden_layer_sizes = hidden_layer_sizes)\n",
    "\n",
    "# fit (train) the model with training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "loss_history = model.loss_curve_\n",
    "\n",
    "# calculate the model response on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#plot true and calculated y values\n",
    "fig, axes = plt.subplots(1,2, figsize=(12, 6))\n",
    "\n",
    "axes[0].plot(loss_history, label=\"loss\")\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].set_ylabel('loss')\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "axes[1].plot(X_test,y_test, \"bo\", label=\"test data\")\n",
    "axes[1].plot(X_test,y_pred, \"o\", color='black', label=\"predicted\")\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "plt.subplots_adjust(bottom=0.15, left=0.05, right=0.95, wspace=0.3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is not prefect, even judging by eye. We can modify the architecture of our network:\n",
    "\n",
    "* number of neurons in hidden layers: `hidden_layer_sizes`\n",
    "* learning rate value: `learning_rate_init`\n",
    "* learning rate scheme: `learning_rate` \n",
    "* number of events in a single batch: `batch_size` \n",
    "* criterium for stopping the learning process: see the `tol` parameter\n",
    "\n",
    "**Please:**\n",
    "\n",
    "* calculate the mean squared error (MSE) metric for the model response on the test data\n",
    "* modify the network architecture until you get `MSE`<$10^{-5}$\n",
    "\n",
    "\n",
    "**Hints:** \n",
    "* use `mean_squared_error()` function from sklearn package\n",
    "* try to understand the loss function behaviour - the loss function is almost equal to MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "...\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Before a ML model will be trained a number of preparations can bne made to the data.\n",
    "A most important one is scaling the inputs to common range. Here we use a class `StandardScaler` to standarize the features:\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "x_{std} = \\frac{x - \\mu_{x}}{\\sigma_{x}}\n",
    "}\n",
    "$$\n",
    "\n",
    "Ussually application of scaling causes the training process to converge faster. We will use a `StandardScaler` class:\n",
    "```Python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_std_train = scaler.fit_transform(X_train)\n",
    "X_std_test = scaler.transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "##BEGIN_SOLUTION\n",
    "hidden_layer_sizes = (128,128,128)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_std_train = scaler.fit_transform(X_train)\n",
    "X_std_test = scaler.transform(X_test)\n",
    "\n",
    "model =  MLPRegressor(random_state=1, max_iter=500, \n",
    "                      hidden_layer_sizes=hidden_layer_sizes,\n",
    "                      learning_rate_init = 1E-4,\n",
    "                      tol=1E-7\n",
    "                     )\n",
    "model.fit(X_std_train, y_train)\n",
    "y_pred = model.predict(X_std_train)\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "print(colored(\"MSE on the training data:\", \"blue\"),mse)\n",
    "\n",
    "loss_history = model.loss_curve_\n",
    "y_pred = model.predict(X_std_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(colored(\"MSE on the test data:\", \"blue\"),mse)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(12, 6))\n",
    "\n",
    "axes[0].plot(loss_history, label=\"loss\")\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].set_ylabel('loss')\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "axes[1].plot(X_test,y_test, \"bo\", label=\"test data\")\n",
    "axes[1].plot(X_test,y_pred, \"o\", color='black', label=\"predicted\")\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "plt.subplots_adjust(bottom=0.15, left=0.05, right=0.95, wspace=0.3)\n",
    "#END_SOLUTION\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KL2QBBWKaiD7"
   },
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukYJSwFYOsTb"
   },
   "source": [
    "[Dataset from the ATLAS Higgs Boson Machine Learning Challenge 2014](https://opendata.cern.ch/record/328)\n",
    "\n",
    "The dataset has been built from official ATLAS full-detector simulation of $pp$ collisions and contains a mixture of events:\n",
    "- $H \\rightarrow \\tau\\tau$, $\\leftarrow$ signal\n",
    "- $Z \\rightarrow \\tau\\tau$, \n",
    "- $Z \\rightarrow ll$\n",
    "- $W \\rightarrow e\\tau$ or $\\mu\\tau$.\n",
    "\n",
    "**Your task is to build a classifier that will be able to select the $H \\rightarrow \\tau\\tau$ events from the background.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xcw-7qGXY9i"
   },
   "source": [
    "**Please:**\n",
    "\n",
    "* dowlonad the data for HiggsBoson_ML_challenge\n",
    "* sample at least 10 000 examples from it (you can use more if you think it will help)\n",
    "* drop unnecessary columns: \n",
    "```Python\n",
    "['KaggleSet','KaggleWeight','Weight', 'EventId']\n",
    "```\n",
    "* prepare labels in digital form: Signal $\\rightarrow$1, Background$\\rightarrow$0. Make sure the labels shape is `(-1,1)`\n",
    "* normalize the input features\n",
    "* split the dataset into training and testing subsets in proportions 8:2\n",
    "* (optional) prepare training dataset in a way that is has the same proprtions of signal and background\n",
    "* print to the screen shapes of training and testing features and labels\n",
    "* create and train a NN classifier using the `MLPClassifier` class\n",
    "* for the **train** data: \n",
    "    * plot loss history \n",
    "    * plot confusion matrix with normalisation that allows to read the FPR and TPR\n",
    "    * print the accuracy\n",
    "* for the **test** data:\n",
    "    * plot confusion matrix with normalisation that allows to read the FPR and TPR\n",
    "    * print the accuracy  \n",
    "\n",
    "\n",
    "**Note:** to get positive mark you have to find a model that gives accuracy at least **0.8** on the training sample, and **0.75** on the test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAi8zaXaas8h"
   },
   "outputs": [],
   "source": [
    "##Cell for loading the data (to be execured once)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##Cell for training and testing a model (to be executed many times)\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wUJ6B-Xskpps",
    "oUEUHPYHw2j8",
    "3m6m_t2x-U2P",
    "WqdB1VTzxlt7",
    "jWheTMxJrEcS",
    "MMTgWagN4jc1",
    "KL2QBBWKaiD7"
   ],
   "include_colab_link": true,
   "name": "mlhep.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
